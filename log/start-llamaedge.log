[2024-09-25 15:40:48.790] [info] rag_api_server in src/main.rs:154: server_version: 0.9.4
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:162: model_name: Qwen1.5-1.8B-Chat-Q5_K_M,nomic-embed-text-v1.5.f16
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:170: model_alias: default,embedding
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:184: ctx_size: 4096,8192
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:198: batch_size: 4096,8192
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:212: prompt_template: chatml,embedding
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:220: n_predict: 1024
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:223: n_gpu_layers: 100
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:236: threads: 2
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:250: rag_prompt: The following text is the context for the user question.\n----------------\n
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:271: qdrant_url: http://127.0.0.1:6333
[2024-09-25 15:40:48.791] [info] rag_api_server in src/main.rs:274: qdrant_collection_name: default
[2024-09-25 15:40:48.792] [info] rag_api_server in src/main.rs:277: qdrant_limit: 3
[2024-09-25 15:40:48.792] [info] rag_api_server in src/main.rs:280: qdrant_score_threshold: 0.5
[2024-09-25 15:40:48.792] [info] rag_api_server in src/main.rs:291: chunk_capacity: 100
[2024-09-25 15:40:48.792] [info] rag_api_server in src/main.rs:294: rag_policy: system-message
[2024-09-25 15:40:48.792] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:434: Initializing the core context for RAG scenarios
[2024-09-25 15:40:48.798] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-25 15:40:48.798] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from Qwen1.5-1.8B-Chat-Q5_K_M.gguf (version GGUF V3 (latest))
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-1.8B-Chat
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 2048
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 5504
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                qwen2.use_parallel_residual bool             = true
[2024-09-25 15:40:48.842] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2
[2024-09-25 15:40:48.877] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
[2024-09-25 15:40:48.899] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 151643
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 151643
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 151643
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:               general.quantization_version u32              = 2
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:                          general.file_type u32              = 17
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:  121 tensors
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_1:   12 tensors
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q8_0:   12 tensors
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_K:  133 tensors
[2024-09-25 15:40:48.933] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q6_K:   13 tensors
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab: missing pre-tokenizer type, using: 'default'
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab:                                             
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab: ************************************        
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab: CONSIDER REGENERATING THE MODEL             
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab: ************************************        
[2024-09-25 15:40:49.052] [warning] [WASI-NN] llama.cpp: llm_load_vocab:                                             
[2024-09-25 15:40:49.110] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 293
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.9338 MB
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = qwen2
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = BPE
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 151936
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 151387
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 32768
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 2048
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 24
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 16
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 16
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 128
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 128
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 128
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 2048
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 2048
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 0.0e+00
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 5504
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 1
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 10000.0
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 32768
[2024-09-25 15:40:49.153] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 1B
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = Q5_K - Medium
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 1.84 B
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 1.28 GiB (5.97 BPW) 
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = Qwen1.5-1.8B-Chat
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 151643 '<|endoftext|>'
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 148848 'ÄĬ'
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOT token        = 151645 '<|im_end|>'
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 256
[2024-09-25 15:40:49.154] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.13 MiB
[2024-09-25 15:40:49.214] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =  1307.33 MiB
[2024-09-25 15:40:49.214] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:40:49.222] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-25 15:40:49.222] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-25 15:40:49.222] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 22 key-value pairs and 112 tensors from nomic-embed-text-v1.5.f16.gguf (version GGUF V3 (latest))
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                          general.file_type u32              = 1
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
[2024-09-25 15:40:49.231] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
[2024-09-25 15:40:49.234] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
[2024-09-25 15:40:49.242] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   51 tensors
[2024-09-25 15:40:49.244] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f16:   61 tensors
[2024-09-25 15:40:49.250] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 5
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.2032 MB
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = nomic-bert
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = WPM
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 30522
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 2048
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 768
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 12
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 12
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 12
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 64
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 64
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 64
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 768
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 768
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 1.0e-12
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-25 15:40:49.251] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 3072
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 1
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 1000.0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 2048
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 137M
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = F16
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 136.73 M
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = nomic-embed-text-v1.5
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 101 '[CLS]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 102 '[SEP]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 100 '[UNK]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: SEP token        = 102 '[SEP]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 0 '[PAD]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: CLS token        = 101 '[CLS]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: MASK token       = 103 '[MASK]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 0 '[PAD]'
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 21
[2024-09-25 15:40:49.252] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.05 MiB
[2024-09-25 15:40:49.262] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =   260.86 MiB
[2024-09-25 15:40:49.262] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:40:49.263] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:489: running mode: rag
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:502: The core context for RAG scenarios has been initialized
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:512: Getting the plugin info
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:592: Getting the plugin info by the graph named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:40:49.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:40:49.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 15:40:49.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:652: Plugin info: b3651(commit 8f1d81a0)
[2024-09-25 15:40:49.263] [info] rag_api_server in src/main.rs:404: plugin_ggml_version: b3651 (commit 8f1d81a0)
[2024-09-25 15:40:49.263] [info] rag_api_server in src/main.rs:414: socket_address: 0.0.0.0:8080
[2024-09-25 15:40:49.263] [info] rag_api_server in src/main.rs:421: gaianet_node_version: 0.4.3
[2024-09-25 15:41:00.506] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:33756, local_addr: 0.0.0.0:8080
[2024-09-25 15:41:00.508] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 99
[2024-09-25 15:41:00.508] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 15:41:00.508] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 15:41:00.508] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 15:41:00.509] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-54e46a9e-3a22-4d40-b9b1-2df5118f40ff
[2024-09-25 15:41:00.509] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 15:41:00.509] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: What is your name?
[2024-09-25 15:41:00.510] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 15:41:00.510] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 15:41:00.510] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:41:00.510] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:41:00.510] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 15:41:00.510] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:41:00.510] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:41:00.510] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:296: Update metadata for the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:41:00.511] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:314: Metadata updated successfully.
[2024-09-25 15:41:00.511] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:41:00.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:41:00.668] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:41:00.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:41:00.669] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:41:00.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:41:00.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:41:00.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:41:00.676] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:41:00.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:41:00.711] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:41:00.711] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:41:00.711] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:41:00.712] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:41:00.712] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:41:00.712] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   12870.26 ms
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    1390.42 ms /     7 tokens (  198.63 ms per token,     5.03 tokens per second)
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:41:02.123] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   12870.77 ms /     8 tokens
[2024-09-25 15:41:02.125] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:41:02.125] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11281
[2024-09-25 15:41:02.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 15:41:02.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:41:02.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 15:41:02.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 7, completion tokens: 0
[2024-09-25 15:41:02.130] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 7 prompt tokens, 0 comletion tokens
[2024-09-25 15:41:02.130] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 15:41:02.130] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 15:41:02.130] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 15:41:02.130] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:41:02.130] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:41:02.130] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 15:41:02.163] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-25 15:41:02.163] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-25 15:41:02.163] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:41:02.163] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-54e46a9e-3a22-4d40-b9b1-2df5118f40ff
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 15:41:02.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:41:02.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:41:03.863] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:41:03.863] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:41:03.863] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:41:03.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:41:03.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:41:03.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:41:03.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:41:03.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:41:03.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-25 15:41:03.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 0
[2024-09-25 15:41:03.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|im_start|>system
Answer as concisely as possible.<|im_end|>
<|im_start|>user
What is your name?<|im_end|>
<|im_start|>assistant
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 15:41:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:41:03.873] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:41:04.001] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:41:04.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:41:04.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:41:04.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:41:04.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:41:04.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:41:04.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-25 15:41:04.009] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:41:04.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:41:04.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:41:25.048] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 15:41:25.054] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:41:25.055] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   28329.26 ms
[2024-09-25 15:41:25.055] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      41.94 ms /    49 runs   (    0.86 ms per token,  1168.45 tokens per second)
[2024-09-25 15:41:25.055] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =   13357.54 ms /    25 tokens (  534.30 ms per token,     1.87 tokens per second)
[2024-09-25 15:41:25.055] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    7457.57 ms /    48 runs   (  155.37 ms per token,     6.44 tokens per second)
[2024-09-25 15:41:25.055] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   35900.64 ms /    73 tokens
[2024-09-25 15:41:25.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:41:25.070] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 212
[2024-09-25 15:41:25.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation: As an AI language model, I do not have a name in the traditional sense. However, if you would like to give me a name, feel free to do so and I will use that name to interact with users on this platform.<|im_end|>
[2024-09-25 15:41:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-25 15:41:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
As an AI language model, I do not have a name in the traditional sense. However, if you would like to give me a name, feel free to do so and I will use that name to interact with users on this platform.
[2024-09-25 15:41:25.073] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:41:25.073] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:41:25.073] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-25 15:41:25.075] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 49
[2024-09-25 15:41:25.075] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 25, completion tokens: 49
[2024-09-25 15:41:25.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-25 15:41:25.085] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-25 15:41:25.085] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 15:41:25.088] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 15:41:25.089] [info] rag_api_server in src/main.rs:517: response_body_size: 516
[2024-09-25 15:41:25.089] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 15:41:25.089] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 15:41:56.571] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:57438, local_addr: 0.0.0.0:8080
[2024-09-25 15:41:56.579] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-25 15:41:56.579] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-25 15:41:56.579] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-25 15:41:56.583] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-25 15:41:56.584] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 15:41:56.584] [info] rag_api_server in src/main.rs:517: response_body_size: 810
[2024-09-25 15:41:56.584] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 15:41:56.584] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 15:42:09.656] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-25 15:42:09.656] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-25 15:42:09.660] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 15:42:09.660] [info] rag_api_server in src/main.rs:517: response_body_size: 1135
[2024-09-25 15:42:09.660] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 15:42:09.660] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 15:42:32.511] [info] rag_api_server in src/main.rs:498: method: OPTIONS, http_version: HTTP/1.1
[2024-09-25 15:42:32.512] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-25 15:42:32.515] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 15:42:32.515] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 15:42:32.515] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 15:42:32.515] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 15:42:32.516] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 15:42:33.087] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 388
[2024-09-25 15:42:33.087] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 15:42:33.087] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 15:42:33.087] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 15:42:33.104] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: 4565bda9-5e5d-4da4-b0a6-77422093209d
[2024-09-25 15:42:33.104] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 15:42:33.104] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: why are traffic rules necessary 
[2024-09-25 15:42:33.105] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 15:42:33.107] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 15:42:33.108] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:42:33.108] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:42:33.109] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 15:42:33.109] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:42:33.109] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:42:33.111] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:42:33.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:42:33.253] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:42:33.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:42:33.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:42:33.265] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:42:33.265] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:42:33.265] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:42:33.270] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:42:33.271] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:42:33.368] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:42:33.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:42:33.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:42:33.369] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:42:33.369] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:42:33.369] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  105573.78 ms
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    1428.09 ms /     7 tokens (  204.01 ms per token,     4.90 tokens per second)
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:42:34.827] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  105574.77 ms /     8 tokens
[2024-09-25 15:42:34.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:42:34.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11250
[2024-09-25 15:42:34.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 15:42:34.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:42:34.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 15:42:34.836] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 7, completion tokens: 0
[2024-09-25 15:42:34.836] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 7 prompt tokens, 0 comletion tokens
[2024-09-25 15:42:34.836] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 15:42:34.837] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 15:42:34.840] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 15:42:34.840] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:42:34.840] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:42:34.840] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 15:42:35.015] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 3
[2024-09-25 15:42:35.017] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 0, score: 0.7227155, source: "    rules = [\n        (\"Always wear a seatbelt while driving\", \"Safety\"),\n        (\"Follow traffic signals and signs\", \"Signals\"),\n        (\"Do not exceed speed limits: 50 km/h in cities, 80 km/h on highways\", \"Speed Limits\"),\n        (\"No honking in silence zones\", \"Offenses\"),\n        (\"Driving without a valid license is illegal\", \"Licensing\"),\n        (\"Helmet is mandatory for both rider and pillion on motorcycles\", \"Safety\")\n    ]\n"
[2024-09-25 15:42:35.017] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 1, score: 0.6121207, source: "# Example usage:\nif __name__ == \"__main__\":\n    create_database()\n    insert_traffic_rules()\n    user_question = \"speed limit\"\n    answer = get_traffic_rule(user_question)\n    print(\"Answer:\", answer)\n"
[2024-09-25 15:42:35.017] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 2, score: 0.5998543, source: "    cursor.execute('''CREATE TABLE IF NOT EXISTS traffic_rules (\n                        rule_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        rule_description TEXT NOT NULL,\n                        rule_category TEXT NOT NULL\n                    )''')\n"
[2024-09-25 15:42:35.017] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:87: Get the chat prompt template type from the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:42:35.018] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:122: prompt_template: chatml
[2024-09-25 15:42:35.019] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:567: rag_policy: system-message
[2024-09-25 15:42:35.019] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:571: context:
"    rules = [\n        (\"Always wear a seatbelt while driving\", \"Safety\"),\n        (\"Follow traffic signals and signs\", \"Signals\"),\n        (\"Do not exceed speed limits: 50 km/h in cities, 80 km/h on highways\", \"Speed Limits\"),\n        (\"No honking in silence zones\", \"Offenses\"),\n        (\"Driving without a valid license is illegal\", \"Licensing\"),\n        (\"Helmet is mandatory for both rider and pillion on motorcycles\", \"Safety\")\n    ]\n"

"# Example usage:\nif __name__ == \"__main__\":\n    create_database()\n    insert_traffic_rules()\n    user_question = \"speed limit\"\n    answer = get_traffic_rule(user_question)\n    print(\"Answer:\", answer)\n"

"    cursor.execute('''CREATE TABLE IF NOT EXISTS traffic_rules (\n                        rule_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        rule_description TEXT NOT NULL,\n                        rule_category TEXT NOT NULL\n                    )''')\n"
[2024-09-25 15:42:35.019] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:575: Merge RAG context into system message.
[2024-09-25 15:42:35.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 15:42:35.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 15:42:35.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-25 15:42:35.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-25 15:42:35.022] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:42:35.022] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:42:35.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: 4565bda9-5e5d-4da4-b0a6-77422093209d
[2024-09-25 15:42:35.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: true
[2024-09-25 15:42:35.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 15:42:35.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 15:42:35.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 15:42:35.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 15:42:35.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:42:35.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:42:35.951] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:42:35.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:42:35.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:42:35.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:42:35.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:42:35.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:42:36.594] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:42:36.594] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:42:36.595] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 98
[2024-09-25 15:42:36.595] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 321, completion tokens: 49
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|im_start|>system
You are a tour guide in Paris, France. Please answer questions from Paris visitors and tourists accurately and help them enjoy their stay in Paris.
The following text is the context for the user question.\n----------------\n
"    rules = [\n        (\"Always wear a seatbelt while driving\", \"Safety\"),\n        (\"Follow traffic signals and signs\", \"Signals\"),\n        (\"Do not exceed speed limits: 50 km/h in cities, 80 km/h on highways\", \"Speed Limits\"),\n        (\"No honking in silence zones\", \"Offenses\"),\n        (\"Driving without a valid license is illegal\", \"Licensing\"),\n        (\"Helmet is mandatory for both rider and pillion on motorcycles\", \"Safety\")\n    ]\n"

"# Example usage:\nif __name__ == \"__main__\":\n    create_database()\n    insert_traffic_rules()\n    user_question = \"speed limit\"\n    answer = get_traffic_rule(user_question)\n    print(\"Answer:\", answer)\n"

"    cursor.execute('''CREATE TABLE IF NOT EXISTS traffic_rules (\n                        rule_id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        rule_description TEXT NOT NULL,\n                        rule_category TEXT NOT NULL\n                    )''')\n"<|im_end|>
<|im_start|>user
why are traffic rules necessary<|im_end|>
<|im_start|>assistant
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 15:42:36.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 15:42:36.599] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:42:36.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:42:37.915] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:42:39.373] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:42:39.373] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:42:39.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:42:39.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:42:39.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:42:39.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-25 15:42:39.591] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-25 15:42:39.591] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 15:42:39.593] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 15:42:39.593] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 15:42:39.593] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 15:42:39.593] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 15:42:39.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 15:42:39.663] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:42:40.789] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 15:42:40.789] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 15:42:40.789] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 15:42:40.790] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 15:42:40.790] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 15:42:40.790] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:42:57.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:57.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:57.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:57.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:57.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:57.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"Traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259177,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:57.833] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:57.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259177,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:57.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:57.953] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:57.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259177,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:57.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" necessary","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.131] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.218] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" many","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.308] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" ways","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.398] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.492] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" safety","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" people","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.859] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:58.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:58.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:58.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:58.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:58.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:58.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:58.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259178,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:58.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.023] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" roads","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.190] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" pedestrians","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.270] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.349] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.443] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" vehicles","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.579] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.733] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Here","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.917] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.917] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" some","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:42:59.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:42:59.997] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:42:59.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:42:59.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:42:59.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:42:59.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:42:59.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" reasons","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259179,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:42:59.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" why","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.255] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.340] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" necessary","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.527] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":":\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"1","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.620] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.702] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.791] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Safety","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.876] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":":","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:00.965] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259180,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:00.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" aim","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.305] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" ensure","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.386] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.476] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" all","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" road","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.659] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" users","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" including","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.905] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" drivers","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:01.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:01.984] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:01.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:01.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:01.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:01.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:01.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259181,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:01.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.073] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" passengers","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.155] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" pedestrians","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.314] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.395] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" cyclists","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.574] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.661] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.662] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.662] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" even","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.662] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" other","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" road","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:02.920] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" users","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259182,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:02.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" such","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.089] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.175] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" motor","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.271] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"cycl","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.355] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"ists","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.526] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" horses","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.616] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.792] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" safely","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.873] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" travel","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:03.960] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:03.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259183,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:03.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" roads","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.209] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" without","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.325] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" causing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.409] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" harm","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.580] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" accidents","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"2","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.844] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:04.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Prevention","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259184,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:04.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.096] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Acc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"idents","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.275] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":":","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.357] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.532] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" establish","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.618] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" guidelines","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.620] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.806] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" behavior","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:05.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:05.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:05.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:05.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:05.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:05.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:05.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259185,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:05.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.063] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" all","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.145] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" road","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" users","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.315] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.400] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" which","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.488] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" helps","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.573] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" prevent","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.663] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" accidents","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.748] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" from","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.832] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" occurring","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:06.995] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259186,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:06.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.083] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" roads","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.249] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"3","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.337] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.429] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Compliance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.515] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" with","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.601] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Laws","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.686] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.773] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Regulations","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.866] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":":","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:07.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:07.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259187,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:07.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" often","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.279] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" accompanied","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.363] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" by","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.442] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" laws","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.525] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" regulations","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.693] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.775] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" apply","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.866] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:08.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:08.950] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:08.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:08.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:08.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:08.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:08.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" driving","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259188,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:08.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.207] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" roads","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.288] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.375] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" Failure","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.463] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" comply","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" with","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" these","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.807] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" laws","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.894] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:09.980] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" regulations","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259189,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:09.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.062] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.151] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" result","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.319] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.320] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.320] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" penalties","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.320] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.408] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" other","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.586] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" legal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.676] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" consequences","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":"In","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:10.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:10.941] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:10.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:10.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:10.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:10.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:10.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" summary","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259190,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:10.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.029] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" traffic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" rules","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.369] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.369] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" necessary","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.450] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" several","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" reasons","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.711] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.797] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.797] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.797] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.797] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.797] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" including","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.884] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" ensuring","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:11.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" safety","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259191,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:11.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.060] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.144] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.247] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" roads","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.329] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.417] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.418] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.418] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" preventing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.418] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.508] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" accidents","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" from","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.679] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" occurring","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.854] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" complying","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:12.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:12.943] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:12.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:12.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:12.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:12.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:12.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" with","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259192,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:12.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.037] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" laws","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.120] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" regulations","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.371] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.466] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" avoiding","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" legal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":" consequences","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 15:43:13.721] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 15:43:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.724] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 15:43:13.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 15:43:13.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 15:43:13.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 99
[2024-09-25 15:43:13.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 321, completion tokens: 185
[2024-09-25 15:43:13.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2845: token_info: 321 prompt tokens, 185 completion tokens
[2024-09-25 15:43:13.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"4565bda9-5e5d-4da4-b0a6-77422093209d","choices":[],"created":1727259193,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk","usage":{"prompt_tokens":321,"completion_tokens":185,"total_tokens":506}}


[2024-09-25 15:43:13.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.728] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 15:43:13.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 15:43:13.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: [DONE]


[2024-09-25 15:43:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 15:43:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2682: Return the chat stream chunk!
[2024-09-25 15:43:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: [GGML] End of sequence
[2024-09-25 15:43:13.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  128429.38 ms
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =     166.14 ms /   186 runs   (    0.89 ms per token,  1119.56 tokens per second)
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =   16792.45 ms /   321 tokens (   52.31 ms per token,    19.12 tokens per second)
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =   15767.02 ms /   184 runs   (   85.69 ms per token,    11.67 tokens per second)
[2024-09-25 15:43:13.731] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  144576.64 ms /   505 tokens
[2024-09-25 15:43:13.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
[2024-09-25 15:52:53.168] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46974, local_addr: 0.0.0.0:8080
[2024-09-25 15:52:53.172] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 156
[2024-09-25 15:52:53.172] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 15:52:53.172] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 15:52:53.172] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 15:52:53.175] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-cc44e500-0d79-47ad-9ee1-1a2d71452420
[2024-09-25 15:52:53.175] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 15:52:53.175] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Hello
[2024-09-25 15:52:53.175] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 15:52:53.175] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 15:52:53.175] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:52:53.175] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:52:53.175] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 15:52:53.176] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:52:53.176] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:52:53.176] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:52:53.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:52:53.345] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:52:53.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:52:53.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:52:53.366] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:52:53.366] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:52:53.366] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:52:53.368] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 15:52:53.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 15:52:53.415] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 15:52:53.415] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 15:52:53.415] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 15:52:53.416] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 15:52:53.416] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 15:52:53.416] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: 
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  727438.19 ms
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3239.84 ms /     3 tokens ( 1079.95 ms per token,     0.93 tokens per second)
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 15:52:56.694] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  727441.77 ms /     4 tokens
[2024-09-25 15:52:56.695] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:52:56.696] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11261
[2024-09-25 15:52:56.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 15:52:56.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 15:52:56.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 15:52:56.707] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 3, completion tokens: 0
[2024-09-25 15:52:56.707] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 3 prompt tokens, 0 comletion tokens
[2024-09-25 15:52:56.707] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 15:52:56.708] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 15:52:56.709] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 15:52:56.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 15:52:56.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 15:52:56.709] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 16:49:55.197] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:49430, local_addr: 0.0.0.0:8080
[2024-09-25 16:49:55.217] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 156
[2024-09-25 16:49:55.217] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 16:49:55.217] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 16:49:55.217] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 16:49:55.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7
[2024-09-25 16:49:55.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 16:49:55.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Hello
[2024-09-25 16:49:55.225] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 16:49:55.225] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 16:49:55.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 16:49:55.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 16:49:55.225] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 16:49:55.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 16:49:55.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 16:49:55.226] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 16:49:55.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 16:49:55.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 16:49:55.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 16:49:55.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 16:49:55.235] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 16:49:55.235] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 16:49:55.395] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 16:49:55.396] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 16:49:55.396] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 16:49:55.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 16:49:55.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 16:49:55.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 16:49:55.430] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 16:49:55.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 16:49:55.471] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 16:49:55.471] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 16:49:55.471] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 16:49:55.472] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 16:49:55.472] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 16:49:55.472] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: 
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 4147451.03 ms
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    1212.40 ms /     3 tokens (  404.13 ms per token,     2.47 tokens per second)
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 16:49:56.706] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 4147453.77 ms /     4 tokens
[2024-09-25 16:49:56.709] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 16:49:56.711] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11261
[2024-09-25 16:49:56.719] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 16:49:56.719] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 16:49:56.720] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 16:49:56.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 3, completion tokens: 0
[2024-09-25 16:49:56.723] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 3 prompt tokens, 0 comletion tokens
[2024-09-25 16:49:56.724] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 16:49:56.725] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 16:49:56.726] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 16:49:56.726] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 16:49:56.726] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 16:49:56.726] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 16:49:56.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-25 16:49:56.983] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-25 16:49:56.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 16:49:56.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 16:49:56.987] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-25 16:49:56.987] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-25 16:49:56.987] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 16:49:56.987] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 16:49:56.987] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7
[2024-09-25 16:49:56.987] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: false
[2024-09-25 16:49:56.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 16:49:56.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 16:49:56.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 16:49:56.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 16:49:56.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 16:49:56.995] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 16:49:57.533] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 16:49:57.533] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 16:49:57.533] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 16:49:57.549] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 16:49:57.549] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 16:49:57.549] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 16:49:57.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 16:49:57.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 16:49:57.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-25 16:49:57.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 19, completion tokens: 0
[2024-09-25 16:49:57.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
[2024-09-25 16:49:57.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-25 16:49:57.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-25 16:49:57.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 16:49:57.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 16:49:57.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 16:49:57.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 16:49:57.645] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 16:49:57.888] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 16:49:57.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 16:49:57.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 16:49:57.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 16:49:57.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 16:49:57.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 16:49:57.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-25 16:49:58.174] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-25 16:49:58.174] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 16:49:58.242] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 16:49:58.242] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 16:49:58.242] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 16:49:58.242] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 16:49:58.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:49:58.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 16:49:58.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 16:49:58.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 16:49:58.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 16:49:58.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 16:49:58.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 16:49:58.770] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 16:49:58.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 16:49:58.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 16:49:58.773] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 16:49:58.773] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 16:49:58.773] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 16:50:14.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.149] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":"Hello","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":"!","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.400] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" How","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.603] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" I","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" help","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.899] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" you","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:14.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:14.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:14.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:14.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:14.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:14.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:14.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" today","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263214,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:14.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":"?","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.205] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" Is","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.291] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.388] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" something","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.487] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" specific","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.575] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" you","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" would","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 16:50:15.790] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 16:50:15.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-2ae85869-8281-47d4-8491-6102280c24f7","choices":[{"index":0,"delta":{"content":" like","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727263215,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 16:50:15.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-25 16:50:15.815] [info] [WASI-NN] llama.cpp: 
[2024-09-25 16:50:15.816] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 4163942.98 ms
[2024-09-25 16:50:15.816] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      17.89 ms /    16 runs   (    1.12 ms per token,   894.10 tokens per second)
[2024-09-25 16:50:15.816] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =   14293.12 ms /    19 tokens (  752.27 ms per token,     1.33 tokens per second)
[2024-09-25 16:50:15.816] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2511.46 ms /    15 runs   (  167.43 ms per token,     5.97 tokens per second)
[2024-09-25 16:50:15.816] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 4166660.64 ms /    34 tokens
[2024-09-25 16:50:15.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
